\documentclass[letterpaper,12pt]{article}
\setlength{\headheight}{15pt}
\setlength{\marginparwidth}{0pt}
\setlength{\marginparsep}{0pt} % width of space between body text and margin notes
\setlength{\evensidemargin}{0.125in} % Adds 1/8 in. to binding side of all 
% even-numbered pages when the "twoside" printing option is selected
\setlength{\oddsidemargin}{0.125in} % Adds 1/8 in. to the left of all pages when "oneside" printing is selected, and to the left of all odd-numbered pages when "twoside" printing is selected
\setlength{\textwidth}{6.375in} % assuming US letter paper (8.5 in. x 11 in.) and side margins as above
\raggedbottom
\setlength{\parskip}{\medskipamount}

\usepackage{amsmath, amsthm, amssymb, fancyhdr, enumitem, tikz}
\pagestyle{fancy}
\lhead{MATH350 --- Lecture 10/31/2022}
\begin{document}
\paragraph{Expectation}
\begin{enumerate}
    \item Suppose a random student is chosen. What is their expected score on a test?
        \paragraph{Ans:}Let $X$ be the score of a randomly chosen student. X takes values 
        $s_1, s_2, \ldots, s_n$.
        \begin{align*}
            E(X) &= s_1 \cdot \frac{1}{n} + s_2 \cdot \frac{1}{n} + \ldots + s_n \cdot \frac{1}{n}\\
                 &= \mathrm{Average~Score}
        \end{align*}
    \item Suppose 5 fair coins are tossed. How many heads are expected? 
    \item Toss a coin 6 times, with $P(H) = \frac{1}{3}$. How many heads are expected?
        \paragraph{Ans:}Let $X$ be the number of heads in 6 tosses, where $P(H) = \frac{1}{3}$. 
        \begin{align*}
            E(X) &= \sum_{k=0}^6 k\cdot\binom{6}{k}\bigg(\frac{1}{3}\bigg)^k\bigg(\frac{2}{3}\bigg)^{6-k}\\
                 &\,\,\vdots\\
                 &= 2
        \end{align*}
    \item Roll a die 96 times. How many 6s are expected?
        \paragraph{Ans:} $$\frac{1}{6} \cdot 96 = 16$$
    \item Roll a die. Let X be the outcome.
        \paragraph{Ans:}
        \begin{align*}
            E(X) &= \sum_{k=1}^6 k\cdot \frac{1}{6}\\
                 &= 3.5
        \end{align*}
\end{enumerate}

\paragraph{Def:}Suppose $X$ is a discrete random variable.
\[
    E(X) = \sum_{k: P(X = k) > 0} k \cdot P(X=k)
\]
\paragraph{Bernoulli:}$$X \sim \mathrm{Ber}(p)$$.
\[ f(x) = \begin{cases}
    0 & p\\
    1 & (1-p)
    \end{cases}
\]
\begin{align*}
    E(X) &= 1\cdot P(X=1) + 0\cdot P(X=0)\\
        &= p
\end{align*}
\paragraph{Binomial:}$$ S_n \sim \mathrm{Bin}(n,p).$$
\paragraph{}Toss $n$ coins, $S_n = \mathrm{Number~of~Heads}$, $P(n) = p$.
$$S_n = X_1 + \ldots + X_n$$
\paragraph{} where $X_i$ are $n$ independent Bernoulli variables.
\begin{align*}
    E(S_n) &= E(X_1 + \ldots + X_n)\\
           &= E(X_1) + E(X_2) + \ldots + E(X_n)\\
           &= np
\end{align*}
\begin{proof}
    \begin{align*}
        E(S_n) &= \sum_{k=0}^n k \cdot \binom{n}{k}\,p^k (1-p)^{n-k}\\
               &= \sum_{k=1}^n \frac{k\cdot n!}{(n-k)! \,k!} \,p^k\,(1-p)^{n-k}\\
               &= \sum_{k=1}^n \frac{n!}{(n-k!)\,(k-1)!}\,p^k\,(1-p)^{n-k}\\
               &= np \sum_{k=1}^n \frac{(n-1)!}{(n-k!)\,(k-1)!}\,p^{k-1}\,(1-p)^{n-k}\\
               &= np \sum_{k=1}^n \binom{n-1}{k-1} \,p^{k-1} \, (1-p)^{n-k}\\
    \end{align*}
    \paragraph{}Let $t = k-1$.
    \begin{align*}
        E(S_n) &=  np \sum_{t=0}^{n-1} \binom{n-1}{t} \,p^{t} \, (1-p)^{n-1-t}\\
    \end{align*}
    $$\sum_{t=0}^{n-1} \binom{n-1}{t} \,p^{t} \, (1-p)^{n-1-t}$$
    \paragraph{}is the sum of the binomial probabilities $\mathrm{Bin}(n-1,p)$.
    \begin{align*}
        E(S_n) &=  np \cdot 1\\
               &= np
    \end{align*}
\end{proof}
    \paragraph{Geometric:}\emph{Calculation in book.}$$X \sim \mathrm{Geom}(p)$$.
\[
    E(x) = \frac{1}{p}
\]

\paragraph{Expectation of a Continuous Random Variable}
\[
    E(x) = \int_{-\infty}^{\infty}x \cdot f(x)\,dx
\]
\paragraph{}\emph{Expectation is also known as the mean, and the first moment.}
\paragraph{Ex:} Let $X \sim U[a,b]$.
\begin{align*}
    E(x) &= \int_{\infty}^{\infty} x\cdot f(x)\,dx\\
         &= \int_b^a x\cdot \frac{1}{b-a}\,dx\\
         &= \frac{x^2}{2(b-a)} \,\bigg|_a^b\\
         &= \frac{b^2-a^2}{2(b-a)}\\
         &= \frac{b+a}{2}
\end{align*}
\paragraph{}This is precisely the center of our uniform density.
\paragraph{Ex:} Roll a fair die. Let $W$ be our winnings in dollars, and $x$ be the number we roll.

\[ W = \begin{cases}
    -1 & x \in \{1,2,3\} \\
    1 & x = 5 \\
    3 & x \in \{5,6\} \\
    \end{cases}
\]

\begin{align*}
    E(W) &= -1\cdot P(W = -1) + 1 \cdot P(W = 1) + 3 \cdot P(W = 3)\\
         &= -1 \cdot \frac{1}{2} + 1\cdot \frac{1}{6} + 3 \cdot \frac{1}{3}\\
\end{align*}

\emph{Alternatively,} let $x = i\,, 1 \le i \le 6$.
\[ W = \begin{cases}
    -1 & x = 1,2,3 \\
    1 & x = 4 \\
    3 & x = 5,6 \\
    \end{cases}
\]
\paragraph{}$W = g(x)$.
\begin{align*}
    &P(W=-1) = P(X=1) + P(X=2) + P(X=3)\\
    &P(W=1) = P(X=4)\\
    &P(W=3) = P(X=5) + P(x=6)
\end{align*}
\paragraph{}Then,
\begin{align*}
    E(W) &= g(1) \cdot P(X=1) + g(2)\cdot P(X=2) + g(3)\cdot P(X=3)\\
         &+ g(4)\cdot P(X=4)\\
         &+ g(5) \cdot P(X=5) + g(6)\cdot P(X=6)\\
         &= E(g(X))
\end{align*}
\paragraph{}Summarily,

\[
    E(g(x)) = \sum_{k=1}^6 g(k)\,P(x=k)
\]
\end{document}



